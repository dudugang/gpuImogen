\documentclass[letterpaper,12pt,twocolumn]{article}
\pagestyle{empty}

\pdfpagewidth 8.5in
\pdfpageheight 11in

\hoffset = 0pt
\oddsidemargin = 0pt
\marginparwidth = 0pt

\voffset = 0pt
\topmargin = 0pt
\headheight = 0pt
\headsep = 12pt

\textwidth = 7in
\textheight = 9.5in
\footskip = 0pt

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}

\author{Erik Keever}
\title{Imogen User's Manual}

\begin{document} 

\maketitle

\section{Setup}

This section will guide you through the Imogen acquisition and setup process.

The setup process consists in making sure that you have the proper libraries
available and informing Imogen of where they are through the Make.config file
in the top level directory. The template Make.config.default should be copied
to Make.config and then setup per instructions below.

\subsection{Download}

Imogen lives online at \url{https://github.com/imogenproject/gpuImogen}

To download it into directory \textit{./gpuimogen} from the public repo,

\textit{ git clone https://github.com/imogenproject/gpuImogen.git ./gpuimogen }

\subsection{Matlab}

Imogen runs in Matlab and so obviously Matlab is a prerequisite to have. A standard
Matlab installation will have everything you need - the parallel computing toolkit
is not needed.

The \textit{MATLAB\_DIR} variable in Make.config must be set to the root of
the Matlab installation (e.g. /opt/Matlab/R2013b). Running Imogen in a different
Matlab version than used here to compile the binary modules is less likely than
might be imagined to cause problems.

\subsection{MPI}

MPI, even if parallel processing is not used, must be available (Imogen still
uses MPI to check that it is running serially), including development headers
to compile the Matlab mex files.

In Make.config, \textit{MPI\_DIR} must refer to the directory containing the include/ folder
that holds the mpi.h file, i.e. the --prefix specified when MPI was installed.

\subsection{Parallel Gateway}

Imogen uses the Parallel Gateway (PGW) library as a go-between with MPI.

Make.config's \textit{PGW\_DIR} variable must be set to the install directory
containing PGW.

\subsection{CUDA}

Imogen's GPU routines are written in CUDA and access to cuda 4.0 or later
libraries is required. The runtime alone isn't sufficient, the SDK is
needed to provide the headers.

Once it is installed set \textit{CUDA\_DIR} to the install location.

Other variables to be set are
\begin{itemize}
\item \textit{CUDA\_LDIR} - either 'lib' on 32bit or 'lib64' on 64bit machines
\item \textit{NVARCH} - \textit{compute\_20} for Fermi, \textit{compute\_30} for Kepler
\item \textit{NVCODE} - \textit{sm\_20} for Fermi, \textit{sm\_30} for Kepler
\end{itemize}

NVARCH and NVCODE are further explained by the nvcc manual page.

\subsection{Compiling}

Once all the above variables are set, run \textit{make} in both the mpi/ and gpuclass/
directories. Both build in parallel so feel free to use -j. 

\subsection{Filesystem}

Imogen will open/create the directory ~/Results. It is not required that this
be a shared filesystem.

\subsection{Cluster access}

Imogen invokes itself in parallel with the './imogen cluster ...' format.

This takes place on lines 95 to 124 of the run/imogen file. Before running
in parallel on a cluster it would be a good idea to examine this section since
it will almost certainly need to be amended, if only to set the package
names to their local values.

\subsection{Troubleshooting}

It is very important that PGW, MPI and the CUDA modules all be compiled
using the same compiler. PGW in particular is insistent about the version
of libfortran it accesses.

Unfortunately this is growing progressively more difficult as CUDA requires
newer GCCs and Mathworks' MEX compiler-wrapper lags.

If Imogen fails because Matlab complains about being unable to resolve an MPI
symbol, the problem is that Matlab defaults to lazy symbol resolution when
loading dynamic libraries and apparently something about the MPI lib trips
up on this. The solution:
\begin{itemize} \item LD\_PRELOAD="\$MPIDIR/lib/libmpi.so"
\end{itemize}
Putting this in your .bashrc (or as sysadmin, in the system's /etc/profile.d) will
make it go away permanently.

\section{Invocation}

Imogen is run through the script run/imogen

There are three separate general types of run:
\begin{itemize}
\item serial - One process uses ones GPU; Directly executed by script
\item parallel - N processes select N GPUs; Direct execution of mpirun by script
\item cluster - N processes on M nodes; Writes script and qsubs it
\end{itemize}

Serial and parallel are for small size simulations that work on a single SMP system,
while the cluster option uses qsub to fire jobs onto large systems.

The invocation syntaxes (also available through ./imogen -help) is as follows:
\begin{itemize}
\item ./imogen serial runfile.m [stream\# [GPU\#]];
\item ./imogen runfile.m; A shortcut that assumes serial, stream 0 and GPU 0.
\item ./imogen parallel runfile.m [stream\#] [NP]
\end{itemize}

\section{Selftest}

\textbf{This is not yet complete -- Erik}

Imogen includes a comprehensive full-simulation test, the simulation
\textit{run\_FullTestSuite.m}. This ``simulation'' will run dozens of test cases
for which an exact solution (or reasonable facimile thereof) is available
and determine whether Imogen is going them right and, where analytic answers
are available, provide convergence orders.

Note that there are a lot of tests to run, and this test falls under the "go have
lunch then check back" class.

\section{Physics solved by the Imogen simulation engine}

\subsection{CFD}

At its core Imogen solves the Euler equations, written here in conservative form:
\begin{align*}
\frac{\partial \rho}{\partial t} &= -\frac{\partial}{\partial x_i} \rho v_i \\
\frac{\partial (\rho v_i)}{\partial t} &= -\frac{\partial}{\partial x_j} (T_{ij} + \mathbb{I} P) \\
\frac{\partial E}{\partial t} &= -\frac{\partial}{\partial x_i} v_i (E + P)
\end{align*}
with mass density $\rho$, fluid bulk velocity $v_i$, momentum stress tensor
$T_{ij} = \rho v_i v_j$, identity tensor $\mathbb{I}$, scalar thermal pressure $P$,
and total energy density $E = \frac{1}{2} \rho v^2 + \epsilon$.

These are closed by the equation of state $P(\epsilon)$. Imogen implements the
adiabatic ideal gas equation of state, $P = (\gamma-1)\epsilon$ for $\gamma > 1$
(physical gas values are $1 < \gamma \le 5/3$).

With the assumption of a gamma law, the energy flux is written into the
code as $v_i \times (\frac{1}{2} \rho v^2 + \frac{\gamma}{\gamma-1}P)$ with $P$ calculated
before fluxing.

\subsubsection{Operator splitting}

Imogen uses a dimensionally split flux solving scheme. Assuming a splittable Hamiltonian
exists in the form
\[\mathbf{H} = \mathbf{E} + \mathbf{F} + \mathbf{G}\]
where in our case, $E$, $F$, and $G$ represent the fluid fluxes in 3 non-degenerate
directions (normally taken as 3 orthogonal directions in a curvilinear coordinate
system: Here, fluxes in the x, y, and z directions), an approximate scheme of the form
\[e^{\epsilon \mathbf{H}} = e^{\epsilon \mathbf{E}} e^{\epsilon \mathbf{F}}
e^{\epsilon \mathbf{G}} + \mathbb{O}(\epsilon^2) \]
always exists, and the reader may easily verify that the local error associated with
doing this with two operators equals their commutator times $\epsilon^2$.

Strang [reference!] showed that by performing such a first order step, then
applying the same operators in exactly reversed order, will cancel all the first-order
errors. This Strang splitting provides an easy way of constructing second spatial order
solvers.

Denoting the operators that solve the inviscid flux equations in the three directions as
$\mathbf{X}$, $\mathbf{Y}$ and $\mathbf{Z}$, there are 6 ways to order them which are the
elements of the permutation group of $\{ \mathbf{X}, \mathbf{Y}, \mathbf{Z} \}$.
In a two-dimensional simulation, there are only two orderings ($XYYX$ and $YXXY$).

Imogen cycles through them iteration to iteration, the idea being that this will average
away (or at least partly so) any systematic errors associated with a given ordering.

There are additional operators associated with MHD (magnetic fluxing operators), non-inertial
frames, radiation, and scalar potental fields (fixed potentials, point objects or
self-gravitation).

The MHD operators are necessarily sprinkled among the fluid update operators. The rest
are mixed in with


\[ eos \]

\textbf{This has been pretty thoroughly tested and should have no code problems.}

\subsection{MHD}

Imogen can simulate magnetic fields in the limit of Ideal MHD where the Euler equations
become:

(ideal mhd equations)

\subsection{Frame rotation}

In many cases a simulation is desired of an object that exhibits coherent rotation at
high speeds.

Imogen's CFL constraint is set by
\[ dt < dt (|v_i| + c_s) \]
so if $v_i$ can be significantly reduced by setting the grid to co-rotate with a flow,
the timestep can be significantly increased for advection-dominated flows with reduced
errors resulting.

The non-inertial sourcing operator introduces its own errors and it is up to the user to
determine the point at which the change is worth it.

This physics is activated by setting a nonzero value for any initializer's
\begin{tt}frameRotateOmega\end{tt} property. The grid
will then rotate about the $\hat{z}$ axis, centered on the X-Y point given in
\begin{tt}run.frameRotateCenter\end{tt}. Positive $\omega$ values cause the frame to turn
counterclockwise as seen from above, i.e. a point on the +X axis with a positive Y momentum
would have its momentum in the rotating frame reduced by a positive $\omega$.

Imogen checks for nonzero rotation at init automatically and makes the appropriate frame
transform by calling

\tt{alterFrameRotation(run, mass, ener, mom, newOmega)}

The user's in-situ analysis function is also free to call it, but be aware, this function
is NOT gpu-accelerated and should be assumed to be Slow.

\subsection{Radiation}

Imogen can solve optically thin radiation with power law cooling,

(power law lambda)

Programming in different cooling laws is extremely simple as a test by making additional
entries to the experiment/Radiation.m file, templatted after the thin power law radiation
one. Of course, don't expect them to be as fast as the GPU accelerated routines.

While the power law cooling is reliable enough in and of itself, it has exhibited a
remarkable ability to derange Imogen's fluid solver routines. None of them has exhibited
the ability to handle an extremely slowly-moving radiating shock front (i.e. the motion
associated with the finite accuracy of the simulation) without some measure of oscillatory
density error at the adiabatic jump.

(Post pics of evil radiative shocks here)

\subsection{Gravity}

\subsubsection{Point gravity}

Imogen allows the addition of massive points to the fluid simulation which obey Newton's
laws. Both point-point and point-fluid gravitational force are computed.

These points, ``compact objects'' as Imogen calls them, are used as standins for stars
or planets which cannot be resolved at reasonably available levels of resolution.

Far away from the object, they behave as point masses. Within a distance defined as
its 'radius', matter is considered as having been accreted. Each step, the mass and
linear and angular momentum in cells whose center is within r of a compact mass
are added to the compact object and replaced with grid's vaccuum values instead.

NOTE: This is not the desired behavior in some cases - e.g. a planet would require a surface
pressure, not a surface vaccuum, in order to be embedded in the disk.

\textbf{Point <-> disk gravity appears to work. Confidence is moderate pending testing.}

\subsubsection{Self gravity}

This is basically dead for now, especially in parallel.

The serial solver could easily enough be resurrected but there's no way I'm going
to have time to setup a parallel Poisson solver, let alone shovel it onto the GPU.

\section{Physics solvers in the initializers}

Many fluid and physics subproblems come up when writing initial condition generators
for Imogen. Some of these have convenient self-contained solvers available within
Imogen.

The available units include:
\begin{itemize}
\item \tt{J = HDJumpSolver(Mach, $\gamma$, $\theta$)}: Returns a structure $J$ describing the
only physical shock solution to a hydrodynamic shock of strength $Mach$ in a fluid of
adiabatic index $\gamma$ and preshock shock-normal flow inclination $\theta$ (in degrees).
\item \tt{J = MHDJumpSolver(Mach, Alfven Mach, $\gamma$, $\theta$)}: Solves the MHD jump
equations for the slowest shock permitted.
\item 
\item \tt{F = fim(X, @f(X))}: Short for Fourier Integral Method, calculates the integral of
anonymous function f on the presumed regularly spaced points $X$ by calculating a polynomial
that renders $f$ 3 times continuous at the endpoints, integrating the polynomial directly
and the residual by Fourier transform.
\item \tt{The ICM}: \tt{icm.m} is currently very much a prototype and not ready for normal use.
It is short for Integral Constraints Method.
\item \tt{t = pade(c\_n, a, b, x)}: Given a polynomial $\sum_n c_n x^n$, calculates the (a,b)
Pade approximant P(a)/Q(b) of the polynomial and evaluates it at x, for any $a+b \le 6$.
\item RadiatingFlowSolver class: Solves the equations of a power-law-radiating hydrodynamic
or magnetohydrodynamic flow. It initializes itself using the 4th (MHD) or 6th (HD) order power
series solution of the flow and then integrates using a 6th order implicit LMM, restarting with
reduced step size when it encounters sharp flow features (knees).
\item Linear wave eigenvectors: The function eigenvectorEntropy, eigenvectorAlfven, eigenvectorSonic,
eigenevectorMA(rho, csq, v, b, k, wavetype) return the linear wave eigenvectors associated with the
flows with the given uniform density, thermal acoustic soundspeed, velocity, magnetic field and wavevector.
Alfven and sonic waves have wavetype +1 for a wave which advances towards $\mathbf{k}$ and -1 for
against $\mathbf{k}$. The MA wavetype can have either sign, with magnitude 2 for a fast MS
wave and magnitude 1 for a slow MS wave. If a nonzero B is passed to a sonic wave, it will invoke the
fast MS solver.
\end{itemize}

\section{Experiments}

\input{../experiment/CentrifugeTest/centrifugeDoc.tex}
\input{../experiment/DoubleBlast/doubleblastDoc.tex}
\input{../experiment/Einfeldt/einfeldtDoc.tex}
\input{../experiment/RichtmyerMeshkov/richtmyermeshkovDoc.tex}


\end{document}
